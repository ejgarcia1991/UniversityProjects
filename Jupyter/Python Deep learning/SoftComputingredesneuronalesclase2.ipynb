{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load data here, and confirm some basic data and see if it loaded correctly and it's complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "white_wine = pd.read_csv('winequality-white.csv',sep=';')\n",
    "red_wine= pd.read_csv('winequality-red.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(white_wine.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wine['type']=1\n",
    "white_wine['type']=0\n",
    "wines=red_wine.append(white_wine,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here onwards, we start the proper modeling and building of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is preparing the training and the test set, in this case we use a holdout for the test equivalent to 33% of the total data. The random_state is set to 42 for consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X=wines.iloc[:,0:12]\n",
    "X=wines.drop('quality',axis=1)\n",
    "Y=np.ravel(wines.quality)\n",
    "\n",
    "\n",
    "X_train=StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is normalized in the following section to avoid giving some attributes bigger importance. This step is important when dealing with numeric data on several algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "seed=7\n",
    "np.random.seed(seed)\n",
    "kfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=seed)\n",
    "for train, test in kfold.split(X,Y):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(128,input_dim=12,activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='SGD',loss='mse',metrics=['mae'])\n",
    "    model.fit(X_train[train],Y[train],epochs=100,verbose=0)\n",
    "    y_pred=model.predict(X_train[test])\n",
    "mse_value,mae_value=model.evaluate(X_train[test],Y[test],verbose=0)\n",
    "r2_scor=r2_score(Y[test],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5356801748275757 0.4731940261979423 0.37735931243664644\n"
     ]
    }
   ],
   "source": [
    "print(mae_value,mse_value,r2_score(Y[test],y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the neural network based on the design and the structure desired.\n",
    "Must have at the very least one input layer, one hidden layer and one output layer.\n",
    "Input must have the structure of the data to be received, in this case we have 12 attributes, which is the amount of neurons we use.\n",
    "Output is only for the quality of the wine, and quality is in the range 0:10, so 10 is used.\n",
    "Hidden layers are flexible and there is no \"right\" setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "model= Sequential() #Define the type of Neural network, Sequential networks move the data in a sequence from input->hidden->output\n",
    "model.add(Dense(12,activation='relu',input_shape=(11,))) #Define the input layer, using \"Dense\" as method, and 'relu' as activation method\n",
    "model.add(Dense(8,activation='relu')) #Define hidden layer 1\n",
    "model.add(Dense(10,activation='tanh')) #Define hidden layer 2\n",
    "model.add(Dense(14,activation='relu')) #Define hidden layer 3\n",
    "model.add(Dense(10,activation='sigmoid')) #Define output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_binary = to_categorical(y_train)\n",
    "\n",
    "model.get_weights()\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X_train,y_binary,epochs=10,batch_size=1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict_classes(X_test)\n",
    "\n",
    "#np.round(y_pred,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary_test = to_categorical(y_test)\n",
    "score=model.evaluate(X_test,y_binary_test,verbose=1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test,y_pred,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test,y_pred,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test,y_pred,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya casi 3 horas después de haber comenzado, creo que tengo la idea básica aprendida, aunque tuve que recurrir bastante a google para lograr definir el tipo de red y predicción como multi-clase.\n",
    "En el caso de ejemplo inicial con el tipo de vino, una comparación binaria tradicional la precisión era excelente, pero ya al querer determinar la calidad del vino se complicó más de lo que hubiera querido. Tampoco estoy del todo familiarizado con Python debido a gran parte del máster ser en R, y me confundo en hacer operaciones de mezcla y separación, selección de atributos y otros, además de que los modelos tienen formato diferente en python, y hay que usar mucho numpy.\n",
    "Forzar capas ocultas intermedias para probar los puntos inferiores mencionados y cambiar de función y usar tanh afectó la precisión final, en fin, es un ejercicio de práctica para entender los fundamentos básicos así que tampoco espero buscar 99.99% de precisión, pero solo 57.61% máximo me hace preguntarme si realmente todo se hizo como se debía."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La parte de los parámetros de las capas se me quedó un poco difusa hasta que fui a Google, realmente los nombres usados en la función no son exactamente claros para alguien aprendiendo, incluso llegué a pensar que el 1 del output layer era porque se daba una única solución y no porque era en el rango 0,1. Luego de estar dando vueltas al código fue que logré entender que son necesarias múltiples neuronas en la capa de salida para dar valores superiores a 1, y fue que pude hacer la parte de clasificación multiclase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
