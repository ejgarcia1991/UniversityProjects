---
title: "Script General de Prueba"
author: "EMAD"
date: "28/3/2020"
output: pdf_document
---

Librerías
```{r}
library(readr)
library(caret)
library(randomForest)
library(e1071)
library(kknn)
library(imbalance)
library(unbalanced)
library(ROSE)
library(smotefamily)
library(NoiseFiltersR)
library(xgboost)
library(ordinal)
library(ordinalForest)
library(nnet)
library(frbs)

library(corrplot)
library(Hmisc)
library(pROC)
library(DMwR)
library(plotrix)
library(RColorBrewer)
library(FSelector)
```

Funciones
```{r}
#Función para medir RMSE
RMSE<-function(predictedValues,realValues){
  x<-as.integer(predictedValues)-1
  y<-as.integer(realValues)-1
  
  sqrt(mean(Mod(x-y)*Mod(x-y)))
}

#Genera DatasetBinarios de una clase contra el resto para poder aplicar SMOTE e incrementar las clases minoritarias
GenerarDatasetBinariosOneClass<-function(df){
  siz<-ncol(df)
  clases<-(as.integer(unique(df[,ncol(df)])))-1 
  aux<-vector("list", length(clases))
  clases<-sort(clases)
  for(x in 1:(length(clases))){
    indices<-which(df[,siz]==clases[x])
    aux[[x]]<-df 
    aux[[x]][,siz]<-as.integer(aux[[x]][,siz])
    aux[[x]][indices,siz]<-1 
    aux[[x]][,siz]<-ifelse(aux[[x]][,siz]==1,1,0)
    aux[[x]][,siz]<-as.factor(aux[[x]][,siz])
  }
  return(aux)
}

#Se hace un oversampling previo a todo el dataset de las clases minoritarias usando MWMOTE y Neater.
OversampleData<-function(dataset){
  oversampledDataset<-dataset
  binariosOneClass<-GenerarDatasetBinariosOneClass(oversampledDataset)
  siz<-ncol(oversampledDataset)
  #------------------
  #Clase 0, 22 casos
  #------------------
  train<-binariosOneClass[[1]]
  
  
  set.seed(seed)
  synth<-imbalance::mwmote(train,numInstances=120,kNoisy = 5,kMajority =3 ,threshold = 5,cmax = 2,cclustering = 3,classAttr = "y")
  synth<-imbalance::neater(train,synth,k=3,iterations=200,classAttr = "y")
  synth$y<-as.integer(synth$y)-1
  synth[,siz]<-0
  synth$y<-as.factor(synth$y)
  
  oversampledDataset<-rbind(oversampledDataset,synth)
  
  #------------------
  #Clase 1, 141 casos
  #------------------
  train<-binariosOneClass[[2]]
  
  set.seed(seed)
  synth<-imbalance::mwmote(train,numInstances=175,kNoisy = 5,kMajority =3 ,threshold = 5,cmax = 2,cclustering = 3,classAttr = "y")
  synth<-imbalance::neater(train,synth,k=3,iterations=200,classAttr = "y")
  synth$y<-as.integer(synth$y)-1
  synth[,siz]<-1
  synth$y<-as.factor(synth$y)
  
  oversampledDataset<-rbind(oversampledDataset,synth)
  
  #------------------
  #Clase 7, 89 casos
  #------------------
  train<-binariosOneClass[[8]]
  
  set.seed(seed)
  synth<-imbalance::mwmote(train,numInstances=110,kNoisy = 5,kMajority =3 ,threshold = 5,cmax = 2,cclustering = 3,classAttr = "y")
  synth<-imbalance::neater(train,synth,k=3,iterations=200,classAttr = "y")
  synth$y<-as.integer(synth$y)-1
  synth[,siz]<-7
  synth$y<-as.factor(synth$y)
  
  oversampledDataset<-rbind(oversampledDataset,synth)
  
  #------------------
  #Clase 8, 46 casos
  #------------------
  train<-binariosOneClass[[9]]
  
  set.seed(seed)
  synth<-imbalance::mwmote(train,numInstances=105,kNoisy = 5,kMajority =3 ,threshold = 5,cmax = 2,cclustering = 3,classAttr = "y")
  synth<-imbalance::neater(train,synth,k=3,iterations=200,classAttr = "y")
  synth$y<-as.integer(synth$y)-1
  synth[,siz]<-8
  synth$y<-as.factor(synth$y)
  
  oversampledDataset<-rbind(oversampledDataset,synth)
  
  #------------------
  #Clase 9, 22 casos
  #------------------
  train<-binariosOneClass[[9]]
  
  set.seed(seed)
  synth<-imbalance::mwmote(train,numInstances=95,kNoisy = 5,kMajority =3 ,threshold = 5,cmax = 2,cclustering = 3,classAttr = "y")
  synth<-imbalance::neater(train,synth,k=3,iterations=200,classAttr = "y")
  
  synth$y<-as.integer(synth$y)-1
  synth[,siz]<-9
  synth$y<-as.factor(synth$y)
  
  oversampledDataset<-rbind(oversampledDataset,synth)
  
  return(oversampledDataset)  
}

#Función wrapper parametrizada que se utiliza posteriormente y devuelve un dataframe con el RMSE de los algoritmos entrenados sobre el dataset aplicando diferentes técnicas según los parámetros usados
testSuite<-function(train,test,useOversample,useOVA,useOVO,rfparams,svmparams,knnparams,xgparams,ordforparams,clmparams,nnetparams,frbsparams,fuzzy.control=NULL){
  trainor<-train
  testor<-test
  if(useOVA){
  OVADataset<-GenerarDatasetBinariosOVA(trainor)
  }
  if(useOVO){
  OVODataset<-GenerarDatasetBinariosOVO(trainor)
  }
  if(useOversample){
  trainor$y<-as.factor(trainor$y)
  Oversampledtrain<-OversampleData(trainor)
  if(class(train$y)=="numeric"){
    Oversampledtrain$y<-as.numeric(Oversampledtrain$y)-1
    trainor$y<-as.numeric(trainor$y)-1
  }
  }
  df<-data.frame(cbind("Sample","3.33"))
  df[,1]<-as.character(df[,1])
  df[,2]<-as.character(df[,2])
  colnames(df)<-c("Algorithm","RMSE")
  
if(rfparams[1]=="TRUE"){
#------------------
#NORMAL RF
#------------------
set.seed(seed)
model<-randomForest::randomForest(y~.,trainor,ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
predictedValues<-predict(model,newdata=testor)
if(class(train$y)=="numeric"){
predictedValues<-round(predictedValues)
}
df<-rbind(df,c("Normal Random Forest", RMSE(predictedValues,testor$y)))
if(useOversample){
#------------------
#Oversampled NORMAL RF
#------------------
set.seed(seed)
model<-randomForest::randomForest(y~.,Oversampledtrain,ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
predictedValues<-predict(model,newdata=testor)
if(class(train$y)=="numeric"){
predictedValues<-round(predictedValues)
}
df<-rbind(df,c("Oversampled Normal Random Forest", RMSE(predictedValues,testor$y)))
}
if(useOVA){
#------------------
#Ordinal OVA RF
#------------------
set.seed(seed)
models<-EntrenarModelosOVA(OVADataset,preprocess = FALSE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
predictedValues<-OrdinalPredictUsingModels(models,testor,clases)
df<-rbind(df,c("Ordinal OVA Random Forest", RMSE(predictedValues,testor$y)))

#------------------
#Processed Ordinal OVA RF
#------------------
set.seed(seed)
models<-EntrenarModelosOVA(OVADataset,preprocess = TRUE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
predictedValues<-OrdinalPredictUsingModels(models,testor,clases)
df<-rbind(df,c("Processed Ordinal OVA Random Forest", RMSE(predictedValues,testor$y)))

#------------------
#Additive OVA RF
#------------------
set.seed(seed)
models<-EntrenarModelosOVA(OVADataset,preprocess = FALSE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
predictedValues<-OVAPredictUsingModels(models,testor,clases)
df<-rbind(df,c("Additive OVA Random Forest", RMSE(predictedValues,testor$y)))

#------------------
#Processed Additive OVA RF
#------------------
set.seed(seed)
models<-EntrenarModelosOVA(OVADataset,preprocess=TRUE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
predictedValues<-OVAPredictUsingModels(models,testor,clases)
df<-rbind(df,c("Processed Additive OVA Random Forest", RMSE(predictedValues,testor$y)))
}
if(useOVO){
#------------------
#OVO REGRESSION RF
#------------------
if(class(train$y)=="numeric"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = FALSE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
  predictedValues<-OVOPredictUsingModelsRegression(models,testor,clases)
  df<-rbind(df,c("OVO regression Random Forest", RMSE(predictedValues,testor$y)))
}

#------------------
#OVO Clasificacion RF Votos
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = FALSE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
  predictedValues<-OVOPredictUsingModelsClassificationVotes(models,testor,clases)
  df<-rbind(df,c("OVO vote based classification Random Forest", RMSE(predictedValues,testor$y)))
}

#------------------
#OVO Clasificacion RF prob
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = FALSE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
  predictedValues<-OVOPredictUsingModelsClassificationProb(models,testor,clases)
  df<-rbind(df,c("OVO probability based classification Random Forest", RMSE(predictedValues,testor$y)))
}

#------------------
#OVO REGRESSION RF
#------------------
if(class(train$y)=="numeric"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = FALSE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
  predictedValues<-OVOPredictUsingModelsRegression(models,testor,clases)
  df<-rbind(df,c("OVO regression Random Forest", RMSE(predictedValues,testor$y)))
}

#------------------
#OVO Clasificacion RF Votos
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = FALSE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
  predictedValues<-OVOPredictUsingModelsClassificationVotes(models,testor,clases)
  df<-rbind(df,c("OVO vote based classification Random Forest", RMSE(predictedValues,testor$y)))
}

#------------------
#OVO Clasificacion RF prob
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = FALSE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
  predictedValues<-OVOPredictUsingModelsClassificationProb(models,testor,clases)
  df<-rbind(df,c("OVO probability based classification Random Forest", RMSE(predictedValues,testor$y)))
}

#------------------
#Processed OVO REGRESSION RF
#------------------
if(class(train$y)=="numeric"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = TRUE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
  predictedValues<-OVOPredictUsingModelsRegression(models,testor,clases)
  df<-rbind(df,c("OVO regression Random Forest", RMSE(predictedValues,testor$y)))
}

#------------------
#Processed OVO Clasificacion RF Votos
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = TRUE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
  predictedValues<-OVOPredictUsingModelsClassificationVotes(models,testor,clases)
  df<-rbind(df,c("Processed OVO vote based classification Random Forest", RMSE(predictedValues,testor$y)))
}

#------------------
#Processed OVO Clasificacion RF prob
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = TRUE,"randomForest",ntree=as.integer(rfparams[2]), mtry=as.integer(rfparams[3]), nodesize = as.integer(rfparams[4]))
  predictedValues<-OVOPredictUsingModelsClassificationProb(models,testor,clases)
  df<-rbind(df,c("Processed OVO probability based classification Random Forest", RMSE(predictedValues,testor$y)))
}
}
}
if(svmparams[1]=="TRUE"){
#------------------
#NORMAL SVM
#------------------
set.seed(seed)
model<-svm(y~.,trainor,kernel=svmparams[2],degree=as.integer(svmparams[3]),coef0=as.numeric(svmparams[4]),epsilon=as.numeric(svmparams[5]),probability=TRUE)
predictedValues<-predict(model,testor)
if(class(train$y)=="numeric"){
  predictedValues<-round(predictedValues)
}
df<-rbind(df,c("Normal SVM", RMSE(predictedValues,testor$y)))
if(useOversample){
#------------------
#Oversampled NORMAL SVM
#------------------
set.seed(seed)
model<-svm(y~.,Oversampledtrain,kernel=svmparams[2],degree=as.integer(svmparams[3]),coef0=as.numeric(svmparams[4]),epsilon=as.numeric(svmparams[5]),probability=TRUE)
predictedValues<-predict(model,testor)
if(class(train$y)=="numeric"){
predictedValues<-round(predictedValues)
}
df<-rbind(df,c("Oversampled Normal SVM", RMSE(predictedValues,testor$y)))
}
if(useOVA){
#------------------
#Ordinal OVA SVM
#------------------
set.seed(seed)
models<-EntrenarModelosOVA(OVADataset,preprocess = FALSE,"svm",kernel=svmparams[2],degree=as.integer(svmparams[3]),coef0=as.numeric(svmparams[4]),epsilon=as.numeric(svmparams[5]),probability=TRUE)
predictedValues<-SVMOrdinalPredict(models,testor,clases)
df<-rbind(df,c("Ordinal OVA SVM", RMSE(predictedValues,testor$y)))

#------------------
#Processed Ordinal OVA SVM
#------------------
set.seed(seed)
models<-EntrenarModelosOVA(OVADataset,preprocess = TRUE,"svm",kernel=svmparams[2],degree=as.integer(svmparams[3]),coef0=as.numeric(svmparams[4]),epsilon=as.numeric(svmparams[5]),probability=TRUE)
predictedValues<-SVMOrdinalPredict(models,testor,clases)
df<-rbind(df,c("Processed Ordinal OVA SVM", RMSE(predictedValues,testor$y)))

#------------------
#Additive OVA SVM
#------------------
set.seed(seed)
models<-EntrenarModelosOVA(OVADataset,preprocess = FALSE,"svm",kernel=svmparams[2],degree=as.integer(svmparams[3]),coef0=as.numeric(svmparams[4]),epsilon=as.numeric(svmparams[5]),probability=TRUE)
predictedValues<-OVAPredictUsingModels(models,testor,clases)
df<-rbind(df,c("Additive OVA SVM", RMSE(predictedValues,testor$y)))

#------------------
#Processed Additive OVA SVM
#------------------
set.seed(seed)
models<-EntrenarModelosOVA(OVADataset,preprocess = TRUE,"svm",kernel=svmparams[2],degree=as.integer(svmparams[3]),coef0=as.numeric(svmparams[4]),epsilon=as.numeric(svmparams[5]),probability=TRUE)
predictedValues<-OVAPredictUsingModels(models,testor,clases)
df<-rbind(df,c("Processed Additive OVA SVM", RMSE(predictedValues,testor$y)))
}
if(useOVO){
#------------------
#OVO REGRESSION SVM
#------------------
if(class(train$y)=="numeric"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = FALSE,"svm",kernel=svmparams[2],degree=as.integer(svmparams[3]),coef0=as.numeric(svmparams[4]),epsilon=as.numeric(svmparams[5]),probability=TRUE)
  predictedValues<-OVOPredictUsingModelsRegression(models,testor,clases)
  df<-rbind(df,c("OVO regression SVM", RMSE(predictedValues,testor$y)))
}

#------------------
#OVO Clasificacion SVM votes
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = FALSE,"svm",kernel=svmparams[2],degree=as.integer(svmparams[3]),coef0=as.numeric(svmparams[4]),epsilon=as.numeric(svmparams[5]),probability=TRUE)
  predictedValues<-OVOPredictUsingModelsClassificationVotes(models,testor,clases)
  df<-rbind(df,c("OVO vote based classification SVM", RMSE(predictedValues,testor$y)))
}

#------------------
#OVO Clasificacion SVM prob
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = FALSE,"svm",kernel=svmparams[2],degree=as.integer(svmparams[3]),coef0=as.numeric(svmparams[4]),epsilon=as.numeric(svmparams[5]),probability=TRUE)
  predictedValues<-SVMOVOPredictUsingModelsClassificationProb(models,testor,clases)
  df<-rbind(df,c("OVO probability based classification SVM", RMSE(predictedValues,testor$y)))
}

#------------------
#Processed OVO REGRESSION SVM
#------------------
if(class(train$y)=="numeric"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = TRUE,"svm",kernel=svmparams[2],degree=as.integer(svmparams[3]),coef0=as.numeric(svmparams[4]),epsilon=as.numeric(svmparams[5]),probability=TRUE)
  predictedValues<-OVOPredictUsingModelsRegression(models,testor,clases)
  df<-rbind(df,c("Processed OVO regression SVM", RMSE(predictedValues,testor$y)))
}

#------------------
#Processed OVO Clasificacion SVM votes
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = TRUE,"svm",kernel=svmparams[2],degree=as.integer(svmparams[3]),coef0=as.numeric(svmparams[4]),epsilon=as.numeric(svmparams[5]),probability=TRUE)
  predictedValues<-OVOPredictUsingModelsClassificationVotes(models,testor,clases)
  df<-rbind(df,c("Processed OVO vote based classification SVM", RMSE(predictedValues,testor$y)))
}

#------------------
#Processed OVO Clasificacion SVM prob
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = TRUE,"svm",kernel=svmparams[2],degree=as.integer(svmparams[3]),coef0=as.numeric(svmparams[4]),epsilon=as.numeric(svmparams[5]),probability=TRUE)
  predictedValues<-SVMOVOPredictUsingModelsClassificationProb(models,testor,clases)
  df<-rbind(df,c("Processed OVO probability based classification SVM", RMSE(predictedValues,testor$y)))
}
}

}
if(knnparams[1]=="TRUE"){
#------------------
#NORMAL KNN
#------------------
set.seed(seed)
model<-kknn(y~.,trainor,testor,k=as.integer(knnparams[2]),distance=as.integer(knnparams[3]),ykernel=as.integer(knnparams[4]))
predictedValues<-model$fitted.values
if(class(train$y)=="numeric"){
predictedValues<-round(predictedValues)
}
df<-rbind(df,c("Normal KNN", RMSE(predictedValues,testor$y)))
if(useOversample){
#------------------
#Oversampled NORMAL KNN
#------------------
set.seed(seed)
model<-kknn(y~.,Oversampledtrain,testor,k=as.integer(knnparams[2]),distance=as.integer(knnparams[3]),ykernel=as.integer(knnparams[4]))
predictedValues<-model$fitted.values
if(class(train$y)=="numeric"){
predictedValues<-round(predictedValues)
}
df<-rbind(df,c("Oversampled Normal KNN", RMSE(predictedValues,testor$y)))
}
if(useOVA){
#------------------
#Ordinal OVA KNN
#------------------
set.seed(seed)
models<-EntrenarModelosOVA(OVADataset,preprocess = FALSE,"kknn",testor,k=as.integer(knnparams[2]),distance=as.integer(knnparams[3]),ykernel=as.integer(knnparams[4]))
predictedValues<-KNNOrdinalPredict(models,testor,clases)
df<-rbind(df,c("Ordinal OVA KNN", RMSE(predictedValues,testor$y)))

#------------------
#Processed Ordinal OVA KNN
#------------------
set.seed(seed)
models<-EntrenarModelosOVA(OVADataset,preprocess = TRUE,"kknn",testor,k=as.integer(knnparams[2]),distance=as.integer(knnparams[3]),ykernel=as.integer(knnparams[4]))
predictedValues<-KNNOrdinalPredict(models,testor,clases)
df<-rbind(df,c("Processed Ordinal OVA KNN", RMSE(predictedValues,testor$y)))

#------------------
#Additive OVA KNN
#------------------
set.seed(seed)
models<-EntrenarModelosOVA(OVADataset,preprocess = FALSE,"kknn",testor,k=as.integer(knnparams[2]),distance=as.integer(knnparams[3]),ykernel=as.integer(knnparams[4]))
predictedValues<-KNNOVAPredict(models,testor,clases)
df<-rbind(df,c("Additive OVA KNN", RMSE(predictedValues,testor$y)))

#------------------
#Processed Additive OVA Oversampled NORMAL KNN
#------------------
set.seed(seed)
models<-EntrenarModelosOVA(OVADataset,preprocess = TRUE,"kknn",testor,k=as.integer(knnparams[2]),distance=as.integer(knnparams[3]),ykernel=as.integer(knnparams[4]))
predictedValues<-KNNOVAPredict(models,testor,clases)
df<-rbind(df,c("Processed Additive OVA KNN", RMSE(predictedValues,testor$y)))
}
if(useOVO){

#------------------
#OVO REGRESSION KNN
#------------------
if(class(train$y)=="numeric"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = FALSE,"kknn",testor,k=as.integer(knnparams[2]),distance=as.integer(knnparams[3]),ykernel=as.integer(knnparams[4]))
  predictedValues<-OVOPredictUsingModelsRegression(models,testor,clases,FALSE)
  df<-rbind(df,c("OVO regression KNN", RMSE(predictedValues,testor$y)))
}

#------------------
#OVO Clasificacion KNN votes
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = FALSE,"kknn",testor,k=as.integer(knnparams[2]),distance=as.integer(knnparams[3]),ykernel=as.integer(knnparams[4]))
  predictedValues<-OVOPredictUsingModelsClassificationVotes(models,testor,clases,FALSE)
  df<-rbind(df,c("OVO vote based classification KNN", RMSE(predictedValues,testor$y)))
}

#------------------
#OVO Clasificacion KNN prob
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = FALSE,"kknn",testor,k=as.integer(knnparams[2]),distance=as.integer(knnparams[3]),ykernel=as.integer(knnparams[4]))
  predictedValues<-KNNOVOPredictUsingModelsClassificationProb(models,testor,clases)
  df<-rbind(df,c("OVO probability based classification KNN", RMSE(predictedValues,testor$y)))
}



#------------------
#Processed OVO REGRESSION KNN
#------------------
if(class(train$y)=="numeric"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = TRUE,"kknn",testor,k=as.integer(knnparams[2]),distance=as.integer(knnparams[3]),ykernel=as.integer(knnparams[4]))
  predictedValues<-OVOPredictUsingModelsRegression(models,testor,clases,FALSE)
  df<-rbind(df,c("Processed OVO regression KNN", RMSE(predictedValues,testor$y)))
}

#------------------
#Processed OVO Clasificacion KNN votes
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = TRUE,"kknn",testor,k=as.integer(knnparams[2]),distance=as.integer(knnparams[3]),ykernel=as.integer(knnparams[4]))
  predictedValues<-OVOPredictUsingModelsClassificationVotes(models,testor,clases,FALSE)
  df<-rbind(df,c("Processed OVO vote based classification KNN", RMSE(predictedValues,testor$y)))
}

#------------------
#Processed OVO Clasificacion KNN prob
#------------------
if(class(train$y)=="factor"){
  set.seed(seed)
  models<-EntrenarModelosOVO(OVODataset,preprocess = TRUE,"kknn",testor,k=as.integer(knnparams[2]),distance=as.integer(knnparams[3]),ykernel=as.integer(knnparams[4]))
  predictedValues<-KNNOVOPredictUsingModelsClassificationProb(models,testor,clases)
  df<-rbind(df,c("Processed OVO probability based classification KNN", RMSE(predictedValues,testor$y)))
}
}
}
if(xgparams[1]=="TRUE"){
#------------------
#MONOTONIC classification
#------------------
if(class(train$y)=="factor"){
set.seed(seed)
models<-MonotonicClassificatorModelTraining(trainor)
predictedValues<-monotonicPredictUsingModels(models,testor,clases)
df<-rbind(df,c("Monotonic XGBoost", RMSE(predictedValues,testor$y)))
}
  
#------------------
#Oversampled MONOTONIC classification
#------------------
  if(useOversample){
if(class(train$y)=="factor"){
set.seed(seed)
models<-MonotonicClassificatorModelTraining(Oversampledtrain)
predictedValues<-monotonicPredictUsingModels(models,testor,clases)
df<-rbind(df,c("Oversampled Monotonic XGBoost", RMSE(predictedValues,testor$y)))
}
  }
}
#------------------
#Ordinal forest package
#------------------
if(ordforparams[1]=="TRUE"){
if(class(train$y)=="factor"){

  set.seed(seed)
  model <- ordfor(depvar="y", data=trainor, nsets=as.integer(ordforparams[2]), ntreeperdiv=as.integer(ordforparams[3]),ntreefinal=as.integer(ordforparams[4]), perffunction = "proportional")
  predictedValues<-predict(ordforres,newdata=testor)
  df<-rbind(df,c("Ordinal Forest", RMSE(predictedValues$ypred,testor$y)))
    
#------------------
#Oversampled Ordinal forest package
#------------------
if(useOversample){
  set.seed(seed)
  model <- ordfor(depvar="y", data=Oversampledtrain, nsets=as.integer(ordforparams[2]), ntreeperdiv=as.integer(ordforparams[3]),ntreefinal=as.integer(ordforparams[4]), perffunction = "proportional")
  predictedValues<-predict(ordforres,newdata=testor)
  df<-rbind(df,c("Oversampled Ordinal Forest", RMSE(predictedValues$ypred,testor$y)))
}
}
}
#------------------
#cummulative link models
#------------------
if(clmparams[1]=="TRUE"){
if(class(train$y)=="factor"){

    set.seed(seed)
    model <- clm(y ~ ., data=trainor)
    predictedValues<-predict(model,testor,type="class")
    df<-rbind(df,c("CLM", RMSE(predictedValues$fit,testor$y)))
    
#------------------
#Oversampled cummulative link models
#------------------
if(useOversample){
    set.seed(seed)
    model <- clm(y ~ ., data=Oversampledtrain)
    predictedValues<-predict(model,testor,type="class")
    df<-rbind(df,c("Oversampled CLM", RMSE(predictedValues$fit,testor$y)))
    }
  }
}
#------------------
#Single Layer Neural Network
#------------------
if(nnetparams[1]=="TRUE"){
  if(class(train$y)=="numeric"){
    model<-nnet::multinom(y~.,trainor)
    predictedValues<-predict(model,testor,type="class")
    df<-rbind(df,c("nnet::multinom", RMSE(predictedValues,testor$y)))
    
    #------------------
    #Fit single-hidden-layer neural network, possibly with skip-layer connections.
    #------------------
    model<-nnet::nnet(y~., data=trainor, size = 10, decay = 0.1, maxit = 1000, linout=T)
    predictedValues<-predict(model,testor,type="class")
    df<-rbind(df,c("nnet::nnet", RMSE(predictedValues,testor$y)))
  }
}

  
#------------------
#Fuzzy Rules Based System
#------------------
  if(frbsparams[1]=="TRUE"){
    if(class(trainor$y)=="factor"){
      trainor$y<-as.numeric(trainor$y)
    }else{
      trainor$y<-trainor$y+1
    }
  range.data.input <- apply(trainor, 2, range)
  
  if(is.null(fuzzy.control)){
    model <- frbs.learn(trainor,range.data.input,method.type = frbsparams[2])
  }else{
    model <- frbs.learn(trainor,range.data.input,method.type = frbsparams[2],fuzzy.control)
  }
  predictedValues<-round(predict(model,testor[,-ncol(testor)]))-1
  df<-rbind(df,c("Fuzzy Rules Based System", RMSE(predictedValues,testor$y)))
  }
  
return(df)
}
```
Carga de datos y fijar la semilla de aleatoriedad
```{r}
seed<-156374562

dataset <- read_csv("datos/train.csv")
dataset <- as.data.frame(dataset)
```
Eliminar de ID del conjunto de entrenamiento
```{r}
dataset$id<-NULL
```

Preprocesamiento opcional según tipo a probar, clasificación o regresión, o normalización en la regresión:
```{r}
dataset$y<-as.factor(dataset$y)
dataset[,1:29]<-scale(dataset[,1:29])
```

Selección de variables de forma automática.

FSelector
```{r}
pesos.chi.squared <- FSelector::chi.squared(y ~ ., dataset)
sub.chi.squared <- FSelector::cutoff.k(pesos.chi.squared, 15)
```

Boruta
```{r}
boruta.train <- Boruta(y~., data = dataset, doTrace = 2)
boruta.data <- TentativeRoughFix(boruta.train)
plot(boruta.data, xlab = "", xaxt = "n", main="Boruta var. selection")
lz <- lapply(1:ncol(boruta.data$ImpHistory),function(i)
boruta.data$ImpHistory[is.finite(boruta.data$ImpHistory[,i]),i])
names(lz) <- colnames(boruta.data$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels), at = 1:ncol(boruta.data$ImpHistory), cex.axis = 0.7)
```

VSURF
Procedimiento de selección variable de tres pasos basado en randomForest para problemas supervisados de clasificación y regresión.
```{r}

SV_train <- VSURF(dataset$y ~., data = dataset)
summary(SV_train)
plot(SV_train)
SV_train$varselect.pred

SV_train.thres  <- VSURF_thres(dataset[,1:29], dataset[,30], ntree = 1000, nfor.thres = 50)
SV_train.interp <- VSURF_interp(dataset[,1:29], dataset[,30], vars = SV_train.thres$varselect.thres, 
                                nfor.interp = 25)
SV_train.pred   <- VSURF_pred(dataset[,1:29], dataset[,30], err.interp = SV_train.interp$err.interp, 
                                varselect.interp = SV_train.interp$varselect.interp, nfor.pred = 25)

SV_train.interp.tuned <- tune(SV_train.interp, nsd = 10)
SV_train.interp.tuned$varselect.interp

SV_train.pred.tuned <- VSURF_pred(dataset[,1:29], dataset[,30], err.interp = SV_train.interp$err.interp, 
                                  varselect.interp = SV_train.interp$varselect.interp, nmj = 100)
```

Multicolinearidad
```{r}
multicol<-function(data){
  R<-cor(data, method="spearman")
  l<-eigen(R)$values
  IC<-max(l)/min(l)
  FIV<-diag(solve(R))
  p<-ncol(R)
  Heo<-1-p/sum(1/l)
  
  return(list(IC=IC, FIVs=FIV, indiceHeo=Heo))
}

m = multicol(dataset[, 1:29])
m


names(which(m$FIVs < 5))

m = multicol(dataset[, sel])
m
```

DISTANCE BASED OUTLIERS (LOF)
```{r}
lof.scores <- lofactor(dataTrain, k = 5)
indices.segun.lof.score.ordenados <- order(lof.scores, decreasing = TRUE)
lof.scores.ordenados <- lof.scores[indices.segun.lof.score.ordenados]
lof.scores.ordenados
par(mfrow=c(1,1))
plot(lof.scores.ordenados)

outliers <- indices.segun.lof.score.ordenados[1:30]

dataset <- dataset[-outliers ,]
```

Selección opcional de variables usando multicolineraridad o alguno de los métodos anteriores
```{r}
sel <- c("X_2", "X_4", "X_5", "X_6", "X_9", "X_10", "X_11", "X_15", "X_16", "X_17", "X_20", "X_21", "X_22", "X_23", "X_26", "X_27", "X_28", "y")

dataset<-dataset[,sel]
```

Partición del dataset en 80% train, 20% test, balanceado con caret
```{r}
set.seed(seed) #Se define la semilla de aleatoriedad para asegurar consistencia en el sampling
trainIndex <- createDataPartition(dataset$y, p = .80, list = FALSE, times = 1)
trainor <- dataset[trainIndex,]
testor  <- dataset[-trainIndex,]
if(class(dataset$y)=="factor"){
clases<-(as.integer(unique(dataset$y))-1) #separo las clases, en caso de que el test no tenga suficientes ejemplos
}else{
clases<-unique(dataset$y)  
}
```

Se ordena el dataset de forma ascendiente para facilitar el proceso de clasificación ordinal
```{r}
trainor<-trainor[order(trainor$y),] 
testor<-testor[order(testor$y),]
clases<-sort(clases) #se ordena igualmente las clases de forma ascendiente para que coincida
```

Parametrización de testSuite, el primer parámetro indica si se quiere incluir dicho algoritmo en el test, y con que parámetros se quiere realizar la evaluación según los mencionados en el comentario. Por ejemplo para SVM se realiza un SVM radial con grado 2, coef0=0.2 y epsilon de 0.05. No todos los algoritmos funcionan con todos los tipos de datos, hay algunos que no funcionan si el dataset es de regresión y no de clasificación y no mostrarán resultados.

En el caso de reglas difusas, fuzzy rules based system (frbs), se toma por parámetro el tipo de sistema a utilizar, y, sí se define, la configuración de control de dicho sistema, de lo contrario se toma la configuración por defecto.
```{r}
rfparams<-c("TRUE","100", "12", "2") #use RF pred, ntree, mtry,nodesize
svmparams<-c("TRUE","radial","2","0.2","0.05") #use SVM pred, kernel, degree, coef0, epsilon
knnparams<-c("TRUE","11","1","1") #use Knn pred,k,distance,ykernel
xgparams<-c("TRUE") #use xgboost for monotonic prediction
ordforparams<-c("TRUE","1000","100","5000") #use ordinalforest package ordinalforest, nsets, ntreeperdiv, ntreefinal
clmparams<-c("TRUE") #use CLM
nnetparams<-c("TRUE") #use Neural Network, no config available
frbsparams<-c("TRUE","WM") #use Fuzzy rule based system, method type ,config is special and optional as defined below

#fuzzy.control<-list(num.labels = 7, type.mf = "TRAPEZOID", type.tnorm = "MIN",type.defuz = "COG", type.implication.func = "ZADEH", name = "Sim-0") #change the control according to the fuzzy rule used

useOversample<-FALSE # Si se quiere incluir un análisis sobre datos con Oversampling de las clases minoritarias
useOVA<-FALSE # Si se quiere incluir un análisis de los algoritmos seleccionados con los diferentes modelos OVA
useOVO<-FALSE # Si se quiere incluir un análisis de los algoritmos seleccionados con los diferentes modelos OVO
```

Llamada a la función testSuite con los parámetros establecidos anteriormente
```{r}
#testS<-testSuite(trainor,testor,useOversample,useOVA,useOVO,rfparams,svmparams,knnparams,xgparams,ordforparams,clmparams,nnetparams,frbsparams,fuzzy.control)
testS<-testSuite(trainor,testor,useOversample,useOVA,useOVO,rfparams,svmparams,knnparams,xgparams,ordforparams,clmparams,nnetparams,frbsparams)
testS
```